{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HuggingFace with GPU\n",
    "###### 참고\n",
    "- [Hugging Face - Inference on One GPU](https://huggingface.co/docs/transformers/perf_infer_gpu_one)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "871f9566e970656f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Inference\n",
    "학습된 모델을 불러와 데이터 처리"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d55d3ea54091e0fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CPU and GPU Offload\n",
    "\n",
    "위 KoLLaMa-13b 모델 load 중 에러 발생하여 [여기](https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu) 참고함.\n",
    "\n",
    "ValueError: \n",
    "                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n",
    "                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n",
    "                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n",
    "                        `device_map` to `from_pretrained`. Check\n",
    "                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
    "                        for more details."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89a629d4c61892df"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String is \"['한국어 명령어를 이해하는 오픈소스 언어모델', '오픈소스를 제공하는 사이트 목록']\"\n",
      "Model Input is \"{'input_ids': tensor([[ 3243,   355, 13263,  6078,  2684,   463,  6609, 39661,  1297, 31962,\n",
      "          2682],\n",
      "        [    0,     0,     0,     0, 16623,   560,  3341, 12673,  7551,  1450,\n",
      "           966]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])}\"\n",
      "bin C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27516208728e4fce8681e8d82701f430"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 'cpu', 'model.layers.13': 'cpu', 'model.layers.14': 'cpu', 'model.layers.15': 'cpu', 'model.layers.16': 'cpu', 'model.layers.17': 'cpu', 'model.layers.18': 'cpu', 'model.layers.19': 'cpu', 'model.layers.20': 'cpu', 'model.layers.21': 'cpu', 'model.layers.22': 'cpu', 'model.layers.23': 'cpu', 'model.layers.24': 'cpu', 'model.layers.25': 'cpu', 'model.layers.26': 'cpu', 'model.layers.27': 'cpu', 'model.layers.28': 'cpu', 'model.layers.29': 'cpu', 'model.layers.30': 'cpu', 'model.layers.31': 'cpu', 'model.norm': 'cpu', 'lm_head': 'cpu'}\n",
      "\n",
      "===================================== Output =====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 명령어를 이해하는 오픈소스 언어모델에 대한 자세한 내용은 [https://www.\n",
      "오픈소스를 제공하는 사이트 목록을 생성합니다.\n",
      "    \n",
      "Allocated GPU Memory: 0.01GB\n",
      "Reserved GPU Memory: 0.02GB\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "try:\n",
    "\tinput_string = [\"한국어 명령어를 이해하는 오픈소스 언어모델\", \"오픈소스를 제공하는 사이트 목록\"]\n",
    "\tprint(f\"Input String is \\\"{input_string}\\\"\")\n",
    "\t\n",
    "\t# 왼쪽에 padding을 추가하는 Tokenizer\n",
    "\ttokenizer = AutoTokenizer.from_pretrained(\"beomi/kollama-7b\", padding_side = \"left\")\n",
    "\t\n",
    "\t# input_string tokenizing\n",
    "\tmodel_input = tokenizer(input_string, return_tensors = \"pt\", return_token_type_ids = False, padding = True)\n",
    "\tprint(f\"Model Input is \\\"{model_input}\\\"\")\n",
    "\t\n",
    "\t# cpu와 gpu를 모두 사용해 효율적으로 추론하도록 auto device_map 설정\n",
    "\tmodel = AutoModelForCausalLM.from_pretrained(\"beomi/kollama-7b\", device_map = \"auto\")\n",
    "\tprint(f\"model.hf_device_map: {model.hf_device_map}\")\n",
    "\t\n",
    "\tprint(f\"\\n===================================== Output =====================================\")\n",
    "\tfor result in tokenizer.batch_decode(model.generate(**model_input), clean_up_tokenization_spaces = True, skip_special_tokens = True ):\n",
    "\t\tprint(result)\n",
    "\t\n",
    "\tdel tokenizer, model_input, model\n",
    "\n",
    "except Exception as e:\n",
    "\tprint(e, e.__traceback__)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Allocated GPU Memory: {torch.cuda.memory_allocated('cuda:0')/(1024**3):.2f}GB\")\n",
    "print(f\"Reserved GPU Memory: {torch.cuda.memory_reserved('cuda:0')/(1024**3):.2f}GB\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:43:40.107521200Z",
     "start_time": "2023-08-03T23:43:04.157100300Z"
    }
   },
   "id": "14eb44e511f27eda"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "02347d5734994e849456aa5f03f5e0e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너 이름이 뭐니 부여 viaalysis 여대생+---------1270SLE974 청년도약계좌693pictureMu별로더럽고AHL693이혼\n",
      "내 자산 맡길 수 있겠니~1애성제 ᅵ 만수목 시곱홍아아\n",
      "애국가 1절 가사옹SET 기자간담회에서 안전사고어부행사 안산병원은 쫓상을QPGDebug 안했는데说话순환을조카\n",
      "오늘 저녁 메뉴는 치킨 끝으로설에 실 *보기이다 실 연토 사랑하는 마음에 표 모 나의 가사\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\"\"\"\n",
    ":arg device_map = \"auto\": 자원할당 옵션 최적화\n",
    ":arg framework = \"pt\": Pytorch, \"tf\" = TensorFlow, default는 설치된 프레임워크 자동 할당인데 혹시 몰라서 씀\n",
    ":arg revision = \"140k\": model의 140k 브랜치\n",
    "\"\"\"\n",
    "pipe = pipeline(\"text-generation\", model = \"beomi/kollama-7b\", device_map = \"auto\", framework = \"pt\", revision = \"140k\")\n",
    "outputs = pipe([\n",
    "\t\"너 이름이 뭐니\",\n",
    "\t\"내 자산 맡길 수 있겠니\",\n",
    "\t\"애국가 1절 가사\",\n",
    "\t\"오늘 저녁 메뉴는 치킨\"\n",
    "])\n",
    "for output in outputs:\n",
    "\tprint(output[0]['generated_text'])\n",
    "\n",
    "del pipe, output\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:45:35.479435900Z",
     "start_time": "2023-08-03T23:43:40.106519900Z"
    }
   },
   "id": "654405a016e9f120"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d83d6ff98f04c289faf349372b7b27d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "너 이름이 뭐니.`n\",\n",
      "       \"     \n",
      "내 자산 맡길 수 있겠니?\\n\",\n",
      "       \"  \n",
      "애국가 1절 가사\\n\",\n",
      "       \"    \n",
      "오늘 저녁 메뉴는 치킨\\n\",\n",
      "       \"     \n"
     ]
    }
   ],
   "source": [
    "# main branch로 다시 해보자\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model = \"beomi/kollama-7b\", device_map = \"auto\", framework = \"pt\")\n",
    "outputs = pipe([\n",
    "\t\"너 이름이 뭐니\",\n",
    "\t\"내 자산 맡길 수 있겠니\",\n",
    "\t\"애국가 1절 가사\",\n",
    "\t\"오늘 저녁 메뉴는 치킨\"\n",
    "])\n",
    "for output in outputs:\n",
    "\tprint(output[0]['generated_text'])\n",
    "\n",
    "del pipe, output\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:47:28.151933200Z",
     "start_time": "2023-08-03T23:45:35.481436100Z"
    }
   },
   "id": "f91914b537d740c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Inference with KoLLaMa-7b"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e234204e8f7c0bde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 변수 선언 및 메모리 정리 함수 정의"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b58ef33f321c6ed"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================== GPU Memory ======================== \n",
      "Allocated GPU Memory: 0.01GB\n",
      "Reserved GPU Memory: 0.02GB\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "from typing import Any\n",
    "\n",
    "# 모델 추론 시 사용할 변수는 전부 inference_dict에 저장\n",
    "inference_dict = dict({})\n",
    "\n",
    "model_name = \"beomi/kollama-7b\"\n",
    "\n",
    "def clear():\n",
    "    # 변수 제거\n",
    "    inference_dict.clear()\n",
    "    # inference_dict[\"inference_input\"] = \"구글에 파이썬 검색\"\n",
    "\t\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n======================== GPU Memory ======================== \")\n",
    "    print(f\"Allocated GPU Memory: {torch.cuda.memory_allocated('cuda:0')/(1024**3):.2f}GB\")\n",
    "    print(f\"Reserved GPU Memory: {torch.cuda.memory_reserved('cuda:0')/(1024**3):.2f}GB\")\n",
    "    \n",
    "clear()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:47:28.251563300Z",
     "start_time": "2023-08-03T23:47:28.154934100Z"
    }
   },
   "id": "a9a0db9701470cf7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pipeline Inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c491673a7eb9a9cc"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def _pipeInference() -> Any:\n",
    "    inference_dict[\"pipe\"] = pipeline(\"text-generation\",\n",
    "                                      model = model_name,\n",
    "                                      device_map = \"auto\",\n",
    "                                      # model_kwargs = model_kwargs,\n",
    "                                      framework = \"pt\"\n",
    "                                      )\n",
    "    \n",
    "    return inference_dict[\"pipe\"](inference_dict[\"inference_input\"])\n",
    "\n",
    "def pipeInference():\n",
    "\ttry:\n",
    "\t\tretVal = _pipeInference()\n",
    "\texcept Exception:\n",
    "\t\t;\n",
    "\telse:\n",
    "\t\treturn retVal\n",
    "\tfinally:\n",
    "\t\tclear()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:47:28.252566900Z",
     "start_time": "2023-08-03T23:47:28.250369500Z"
    }
   },
   "id": "40315ee7c3222a26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model Load Inference "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d44bc7734a075f5"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def _modelInference() -> Any:\n",
    "\tinference_dict[\"tokenizer\"] = AutoTokenizer.from_pretrained(model_name,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpadding_side = \"left\")\n",
    "\t\n",
    "\tinference_dict[\"inference_token\"] = inference_dict[\"tokenizer\"](inference_dict[\"inference_input\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treturn_tensors = \"pt\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\treturn_token_type_ids = False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tpadding = True)\n",
    "\t\n",
    "\tprint(f\"\\nInference Token is \\\"{inference_dict['inference_token']}\\\"\")\n",
    "\t\n",
    "\tinference_dict[\"model\"] = AutoModelForCausalLM.from_pretrained(model_name, device_map = \"auto\")\n",
    "\t\n",
    "\t# print(f\"\\nModel\\n===================================================\\n{inference_dict['model']}\")\n",
    "\tprint(f\"model.hf_device_map: {inference_dict['model'].hf_device_map}\")\n",
    "\t\n",
    "\t# Because of OOM, Comment\n",
    "\t# inference_dict[\"keys\"] = inference_dict[\"model\"](**inference_dict[\"inference_token\"]).keys()\n",
    "\t# print(f\"Key Dictionary of Inference Result is \\\"{inference_dict['keys']}\\\"\")\n",
    "\t\n",
    "\t# modelling_llama.py LlamaForCausalLM.forward() line 760 parameter 참고\n",
    "\tinference_dict[\"result_tensor\"] = inference_dict[\"model\"].generate(**inference_dict[\"inference_token\"])\n",
    "\tprint(f\"\\nGeneration Result Tensor is \\\"{inference_dict['result_tensor']}\\\"\")\n",
    "\t\n",
    "\t# modelling_llama.py LlamaForCausalLM.forward() line 795\n",
    "\t# tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\t# inference_dict[\"result_with_sst\"] = inference_dict[\"tokenizer\"].batch_decode(inference_dict[\"result_tensor\"],\n",
    "\t# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t skip_special_tokens = True, # default False\n",
    "\t# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t clean_up_tokenization_spaces = False)\n",
    "\t# print(f\"\\nGeneration Result with Skip_Special_Tokens is \\\"{inference_dict['result_with_sst']}\\\"\")\n",
    "\t# \n",
    "\t# inference_dict[\"result\"] = inference_dict[\"tokenizer\"].batch_decode(inference_dict[\"result_tensor\"],\n",
    "\t# \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t clean_up_tokenization_spaces = True)\n",
    "\t# print(f\"\\nGeneration Result is \\\"{inference_dict['result']}\\\"\")\n",
    "\t\n",
    "\treturn inference_dict[\"tokenizer\"].batch_decode(inference_dict[\"result_tensor\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tclean_up_tokenization_spaces = True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tskip_special_tokens = True)\n",
    "\t\n",
    "\n",
    "\n",
    "def modelInference():\n",
    "\ttry:\n",
    "\t\tretVal = _modelInference()\n",
    "\texcept Exception:\n",
    "\t\t;\n",
    "\telse:\n",
    "\t\treturn retVal\n",
    "\tfinally:\n",
    "\t\tclear()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:47:28.258426300Z",
     "start_time": "2023-08-03T23:47:28.256919500Z"
    }
   },
   "id": "5c6399065101a76f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Input Strings, Results of Pipeline and Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c57cfdda714f935"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Inference...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31dd7ca9bb4147daa04043607e18d6c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================== GPU Memory ======================== \n",
      "Allocated GPU Memory: 0.01GB\n",
      "Reserved GPU Memory: 0.02GB\n",
      "\n",
      "Model Inference...\n",
      "\n",
      "Inference Token is \"{'input_ids': tensor([[    0,     0,     0,     0, 26660,   279, 16862,  6991,   109,  7087],\n",
      "        [13903,  6991,   109,   378, 31143,  1004,   594,   472,  2233,  2409],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,  1338, 10598, 21807],\n",
      "        [    0,     0,     0,     0,   856, 10402, 34973,   422, 11783,   386],\n",
      "        [    0,     0,     0,     0,     0,  1491,  6722,   315,  1310,  9995],\n",
      "        [    0,     0,     0,     0,     0,     0,  4662,  7211, 35772, 18245]]), 'attention_mask': tensor([[0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]])}\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ceff728e15d4e8baaedcc927ddcb9c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 'cpu', 'model.layers.12': 'cpu', 'model.layers.13': 'cpu', 'model.layers.14': 'cpu', 'model.layers.15': 'cpu', 'model.layers.16': 'cpu', 'model.layers.17': 'cpu', 'model.layers.18': 'cpu', 'model.layers.19': 'cpu', 'model.layers.20': 'cpu', 'model.layers.21': 'cpu', 'model.layers.22': 'cpu', 'model.layers.23': 'cpu', 'model.layers.24': 'cpu', 'model.layers.25': 'cpu', 'model.layers.26': 'cpu', 'model.layers.27': 'cpu', 'model.layers.28': 'cpu', 'model.layers.29': 'cpu', 'model.layers.30': 'cpu', 'model.layers.31': 'cpu', 'model.norm': 'cpu', 'lm_head': 'cpu'}\n",
      "\n",
      "Generation Result Tensor is \"tensor([[    0,     0,     0,     0, 26660,   279, 16862,  6991,   109,  7087,\n",
      "           284,  5967,   969,    17,   202,   202,   224,   224,   224,   224],\n",
      "        [13903,  6991,   109,   378, 31143,  1004,   594,   472,  2233,  2409,\n",
      "           279,   991, 13439, 11560,   832,  2413,  1731,  3661,    17, 11145],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,  1338, 10598, 21807,\n",
      "            17,    67,    81,  1019,   202,   224,   224,   224,   224,   224],\n",
      "        [    0,     0,     0,     0,   856, 10402, 34973,   422, 11783,   386,\n",
      "            34,    63,    81,  1019,   202,   224,   224,   224,   224,   224],\n",
      "        [    0,     0,     0,     0,     0,  1491,  6722,   315,  1310,  9995,\n",
      "            63,    81,  1019,   202,   224,   224,   224,   224,   224,   224],\n",
      "        [    0,     0,     0,     0,     0,     0,  4662,  7211, 35772, 18245,\n",
      "            63,    81,  1019,   202,   224,   224,   224,   224,   224,   224]])\"\n",
      "\n",
      "======================== GPU Memory ======================== \n",
      "Allocated GPU Memory: 0.01GB\n",
      "Reserved GPU Memory: 0.02GB\n",
      "['구글에 파이썬 검색', '파이썬으로 가위바위보 게임 개발', '너 이름이 뭐니', '내 자산 맡길 수 있겠니', '애국가 1절 가사', '오늘 저녁 메뉴는 치킨'] [[{'generated_text': '구글에 파이썬 검색을 구현합니다.\\n\\n        '}], [{'generated_text': '파이썬으로 가위바위보 게임 개발에 대한 자세한 내용은 [https://www.youtube'}], [{'generated_text': '너 이름이 뭐니.`n\",\\n       \"     '}], [{'generated_text': '내 자산 맡길 수 있겠니?\\\\n\",\\n       \"  '}], [{'generated_text': '애국가 1절 가사\\\\n\",\\n       \"    '}], [{'generated_text': '오늘 저녁 메뉴는 치킨\\\\n\",\\n       \"     '}]] ['구글에 파이썬 검색을 구현합니다.\\n\\n    ', '파이썬으로 가위바위보 게임 개발에 대한 자세한 내용은 [https://www.youtube', '너 이름이 뭐니.`n\",\\n     ', '내 자산 맡길 수 있겠니?\\\\n\",\\n     ', '애국가 1절 가사\\\\n\",\\n      ', '오늘 저녁 메뉴는 치킨\\\\n\",\\n      ']\n",
      "============================= Inference Result =============================\n",
      "구글에 파이썬 검색, 구글에 파이썬 검색을 구현합니다.\n",
      "\n",
      "        , 구글에 파이썬 검색을 구현합니다.\n",
      "\n",
      "    \n",
      "파이썬으로 가위바위보 게임 개발, 파이썬으로 가위바위보 게임 개발에 대한 자세한 내용은 [https://www.youtube, 파이썬으로 가위바위보 게임 개발에 대한 자세한 내용은 [https://www.youtube\n",
      "너 이름이 뭐니, 너 이름이 뭐니.`n\",\n",
      "       \"     , 너 이름이 뭐니.`n\",\n",
      "     \n",
      "내 자산 맡길 수 있겠니, 내 자산 맡길 수 있겠니?\\n\",\n",
      "       \"  , 내 자산 맡길 수 있겠니?\\n\",\n",
      "     \n",
      "애국가 1절 가사, 애국가 1절 가사\\n\",\n",
      "       \"    , 애국가 1절 가사\\n\",\n",
      "      \n",
      "오늘 저녁 메뉴는 치킨, 오늘 저녁 메뉴는 치킨\\n\",\n",
      "       \"     , 오늘 저녁 메뉴는 치킨\\n\",\n",
      "      \n",
      "\n",
      "======================== GPU Memory ======================== \n",
      "Allocated GPU Memory: 0.01GB\n",
      "Reserved GPU Memory: 0.02GB\n"
     ]
    }
   ],
   "source": [
    "# unused\n",
    "device_map = {\n",
    "\t\"lm_head\": \"cpu\",\n",
    "\t\"model.embed_tokens\": \"cpu\",\n",
    "\t\"model.layers.0\": 0,\n",
    "\t\"model.layers.1\": 0,\n",
    "\t\"model.layers.2\": 0,\n",
    "\t\"model.layers.3\": 0,\n",
    "\t\"model.layers.4\": 0,\n",
    "\t\"model.layers.5\": 0,\n",
    "\t\"model.layers.6\": 0,\n",
    "\t\"model.layers.7\": 0,\n",
    "\t\"model.layers.8\": 0,\n",
    "\t\"model.layers.9\": 0,\n",
    "\t\"model.layers.10\": 0,\n",
    "\t\"model.layers.11\": \"cpu\",\n",
    "\t\"model.layers.12\": \"cpu\",\n",
    "\t\"model.layers.13\": \"cpu\",\n",
    "\t\"model.layers.14\": \"cpu\",\n",
    "\t\"model.layers.15\": \"cpu\",\n",
    "\t\"model.layers.16\": \"cpu\",\n",
    "\t\"model.layers.17\": \"cpu\",\n",
    "\t\"model.layers.18\": \"cpu\",\n",
    "\t\"model.layers.19\": \"cpu\",\n",
    "\t\"model.layers.20\": \"cpu\",\n",
    "\t\"model.layers.21\": \"cpu\",\n",
    "\t\"model.layers.22\": \"cpu\",\n",
    "\t\"model.layers.23\": \"cpu\",\n",
    "\t\"model.layers.24\": \"cpu\",\n",
    "\t\"model.layers.25\": \"cpu\",\n",
    "\t\"model.layers.26\": \"cpu\",\n",
    "\t\"model.layers.27\": \"cpu\",\n",
    "\t\"model.layers.28\": \"cpu\",\n",
    "\t\"model.layers.29\": \"cpu\",\n",
    "\t\"model.layers.30\": \"cpu\",\n",
    "\t\"model.layers.31\": \"cpu\",\n",
    "\t\"model.norm\": \"cpu\",\n",
    "\t\"transformer.h\": 0,\n",
    "\t\"transformer.ln_f\": 0,\n",
    "\t\"transformer.word_embeddings\": 0,\n",
    "\t\"transformer.word_embeddings_layernorm\": 0\n",
    "}\n",
    "\n",
    "inputs = [\n",
    "\t\"구글에 파이썬 검색\",\n",
    "\t\"파이썬으로 가위바위보 게임 개발\",\n",
    "\t\"너 이름이 뭐니\",\n",
    "\t\"내 자산 맡길 수 있겠니\",\n",
    "\t\"애국가 1절 가사\",\n",
    "\t\"오늘 저녁 메뉴는 치킨\"\n",
    "]\n",
    "\n",
    "inference_dict[\"inference_input\"] = inputs\n",
    "print(\"Pipeline Inference...\")\n",
    "pipeline_val = pipeInference()\n",
    "\n",
    "print()\n",
    "\n",
    "inference_dict[\"inference_input\"] = inputs\n",
    "print(\"Model Inference...\")\n",
    "model_val = modelInference()\n",
    "\n",
    "print(inputs, pipeline_val, model_val)\n",
    "\n",
    "print(\"============================= Inference Result =============================\")\n",
    "for (inf_input, pipe_output, model_output) in zip(inputs, pipeline_val, model_val):\n",
    "\tprint(f\"{inf_input}, {pipe_output[0]['generated_text']}, {model_output}\")\n",
    "\t\n",
    "del pipeline_val, model_val\n",
    "clear()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:50:30.361547500Z",
     "start_time": "2023-08-03T23:47:28.261426700Z"
    }
   },
   "id": "bc7dcc81367933c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 결과\n",
    "\n",
    "내 질문에 대답하는 게 아니라 입력한 문자열을 그럴듯하게 포장함.\n",
    "\n",
    "The model is not intended to inform decisions about matters central to human life, and should not be used in such a way.\n",
    "([원본](https://huggingface.co/beomi/kollama-7b#ethical-considerations))\n",
    "\n",
    "반대로 사람이 입력한 문자열에서 키워드만 뽑아서 명령어로 만드는 것도 고려해볼만한듯"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac1d8f336f64d08f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pipeline(task = \"text-generation\")\n",
    "- Model Load\n",
    "\t- Model.from_pretrained()\n",
    "- Tokenize input\n",
    "\t- Tokenizer()\n",
    "- Generate Token Ids\n",
    "\t- Model.generate(tokenized_input)\n",
    "- Decode\n",
    "\t- Tokenizer.batch_decode(generated_ids)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4fa1230a066d2c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Solving \"CUDA out of memory\" Error\n",
    "\n",
    "[참고](https://www.kaggle.com/discussions/getting-started/140636)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d740fe08e31c9d1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  2% |  4% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 13% |  4% |\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()          "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T23:55:47.091141300Z",
     "start_time": "2023-08-03T23:55:46.826883200Z"
    }
   },
   "id": "1f0baaadae9fd5b3"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/*====================================================================*/\n",
      "device:0\n",
      "NVIDIA GeForce RTX 3060\n",
      "\n",
      "/* Before */\n",
      "Allocated GPU Memory: 0.01GB\n",
      "Reserved GPU Memory: 0.02GB\n",
      "\n",
      "/* After */\n",
      "Allocated GPU Memory: 0.01GB\n",
      "Reserved GPU Memory: 0.02GB\n",
      "/*====================================================================*/\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "from numba import cuda\n",
    "\n",
    "def clear_cache(gpu_cnt: int = 1):\n",
    "\t\n",
    "\tfor i in range(gpu_cnt):\n",
    "\t\ttry:\n",
    "\t\t\tcuda_name = torch.cuda.get_device_name(i)\n",
    "\t\t\tprint(\"\\n/*====================================================================*/\")\n",
    "\t\t\tprint(f\"device:{i}\")\n",
    "\t\t\tprint(cuda_name)\n",
    "\t\texcept AssertionError as ae:\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\tprint(\"\\n/* Before */\")\n",
    "\t\tprint(f\"Allocated GPU Memory: {torch.cuda.memory_allocated(i)/(1024**3):.2f}GB\")\n",
    "\t\tprint(f\"Reserved GPU Memory: {torch.cuda.memory_reserved(i)/(1024**3):.2f}GB\")\n",
    "\t\t\n",
    "\t\tgc.collect()\n",
    "\t\ttorch.cuda.empty_cache()\n",
    "\t\t\n",
    "\t\tcuda.select_device(i)\n",
    "\t\tcuda.close()\n",
    "\t\tcuda.select_device(i)\n",
    "\t\t\n",
    "\t\tprint(\"\\n/* After */\")\n",
    "\t\tprint(f\"Allocated GPU Memory: {torch.cuda.memory_allocated(i)/(1024**3):.2f}GB\")\n",
    "\t\tprint(f\"Reserved GPU Memory: {torch.cuda.memory_reserved(i)/(1024**3):.2f}GB\")\n",
    "\t\n",
    "\tprint(\"/*====================================================================*/\")\n",
    "\t\n",
    "clear_cache(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T00:14:15.241167900Z",
     "start_time": "2023-08-04T00:14:15.085717800Z"
    }
   },
   "id": "61a2fc7caf1f927e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

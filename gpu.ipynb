{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HuggingFace with GPU\n",
    "###### 참고\n",
    "- [Hugging Face - Training on One GPU](https://huggingface.co/docs/transformers/perf_train_gpu_one)\n",
    "- [Hugging Face - Inference on One GPU](https://huggingface.co/docs/transformers/perf_infer_gpu_one)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad3a235cf8de9e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Efficient Training on a Single GPU\n",
    "\n",
    "더 적은 메모리 사용, 모델 학습 속도 상승, Trainer와 Accelerate가 통합되는 과정을 살펴본다.\n",
    "\n",
    "아래의 각 method들은 속도 또는 메모리 사용 면에서 향상된다.\n",
    "\n",
    "| Method | Speed | Memory | 비고 |\n",
    "| --- | --- | --- |\n",
    "| Gradient accumulation | No | Yes | 높은 batch size의 부하를 견디기 위해 모든 batch가 gradient를 Global Gradient에 누적시킨 뒤 한 번의 forward pass와 back propagation을 통해 전달 |\n",
    "| Gradient checkpointing | No | Yes | 모델 학습 시 모든 노드의 가중치를 저장하지 않고 check point의 가중치만 저장하여 속도가 느린 대신 메모리 사용량이 줄어듦 |\n",
    "| Mixed precision training | Yes | (No) | FP(Float Precision)16과 FP32를 혼용, FW / BW propagation은 FP16, 가중치를 업데이트하는 과정에서 다시 FP32로 변환해 메모리 사용을 줄이고 변환 과정의 오차로 인한 loss값도 줄임 |\n",
    "| Batch size | Yes | Yes | - |\n",
    "| Optimizer choice | Yes | Yes | - |\n",
    "| DataLoader | Yes | No | - |\n",
    "| Deepspeed Zero | No | Yes | 분산 학습 과정에서의 불필요한 메모리 중복을 제거하여 동시에 학습 가능한 파라미터의 수를 크레 늘릴 수 있는 새로운 병렬 최적화 도구 |\n",
    "\n",
    "### Libraries\n",
    "`pip install transformers datasets accelerate nvidia-ml-py3`\n",
    "\n",
    "- accelerate: 같은 PyTorch 코드라도 4줄만 추가해 더 쉽고 효율적으로 사용할 수 있도록 만들어주는 라이브러리\n",
    "- nvidia-ml-py3: 모델의 메모리 사용량을 파이썬에서 모니터링할 수 있는 라이브러리. 터미널의 nvidia-smi와 유사"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f29cace84313367"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# dummy data\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, dataset_size)\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:45:52.535521200Z",
     "start_time": "2023-08-01T03:45:50.974436400Z"
    }
   },
   "id": "b3d73676ac61b1fb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 343 MB.\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용율 및 Trainer를 사용한 훈련 실행에 대한 요약 통계\n",
    "\n",
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "print_gpu_utilization()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:45:52.570276400Z",
     "start_time": "2023-08-01T03:45:52.536521300Z"
    }
   },
   "id": "fa430cd847b5af6f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1734 MB.\n"
     ]
    }
   ],
   "source": [
    "# Load Model bert-large-uncased model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to(\"cuda\")\n",
    "print_gpu_utilization()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:45:56.179237800Z",
     "start_time": "2023-08-01T03:45:53.910833300Z"
    }
   },
   "id": "8191b6dcdf8c2e42"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# default training args\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:45:56.180744100Z",
     "start_time": "2023-08-01T03:45:56.179237800Z"
    }
   },
   "id": "c1089701ef2e039f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training 함수"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fea30953735fcbf7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.25GB\n",
      "Reserved GPU memory: 1.26GB\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.training_args import OptimizerNames\n",
    "from typing import Union\n",
    "import bitsandbytes as bnb\n",
    "import torch.cuda\n",
    "import gc\n",
    "\n",
    "logging.set_verbosity_error()  # error log activate\n",
    "\n",
    "\n",
    "def get_training_args(\n",
    "        per_device_train_batch_size: int = 8,\n",
    "\t\tgradient_accumulation_steps: int = 1,\n",
    "\t\tgradient_checkpointing: bool = False,\n",
    "\t\toptim: Union[OptimizerNames, str] = \"adamw_hf\",\n",
    "\t\tfloat_precision: str = None,\n",
    "\t\tdeepspeed: str = None,\n",
    ") -> TrainingArguments:\n",
    "\ttraining_args = TrainingArguments(\n",
    "\t\tper_device_train_batch_size = per_device_train_batch_size,\n",
    "\t\tgradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\t\tgradient_checkpointing = gradient_checkpointing,\n",
    "\t\toptim = optim,\n",
    "        deepspeed = deepspeed,\n",
    "\t\t**default_args)\n",
    "\t\n",
    "\tif float_precision == \"fp16\":\n",
    "\t\ttraining_args.fp16 = True\n",
    "\telif float_precision == \"bf16\":\n",
    "\t\ttraining_args.bf16 = True\n",
    "\telif float_precision == \"tf32\":\n",
    "\t\ttraining_args.tf32 = True\n",
    "\t\n",
    "\treturn training_args\n",
    "\n",
    "def clean_training(\n",
    "\t\tper_device_train_batch_size: int = 8,\n",
    "\t\tgradient_accumulation_steps: int = 1,\n",
    "\t\tgradient_checkpointing: bool = False,\n",
    "\t\toptim: Union[OptimizerNames, str] = \"adamw_hf\",\n",
    "\t\tfloat_precision: str = None,\n",
    "\t\tdeepspeed: str = None,\n",
    "        training: bool = True,\n",
    "        use_bnb: bool = False\n",
    "):\n",
    "    if training:\n",
    "        training_args = get_training_args(\n",
    "\t        per_device_train_batch_size = per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            gradient_checkpointing = gradient_checkpointing,\n",
    "            optim = optim,\n",
    "            float_precision = float_precision,\n",
    "            deepspeed = deepspeed\n",
    "\t    )\n",
    "        \n",
    "        if float_precision == \"fp16\":\n",
    "            training_args.fp16 = True\n",
    "        elif float_precision == \"bf16\":\n",
    "            training_args.bf16 = True\n",
    "        elif float_precision == \"tf32\":\n",
    "            training_args.tf32 = True\n",
    "            \n",
    "        # trainer = get_trainer(args = training_args, use_bnb = use_bnb)\n",
    "        \n",
    "        if use_bnb:\n",
    "            decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "            decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "                    \"weight_decay\": training_args.weight_decay\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "                    \"weight_decay\": 0.0\n",
    "                }\n",
    "            ]\n",
    "            adam_bnb_optim = bnb.optim.AdamW8bit(\n",
    "                optimizer_grouped_parameters,\n",
    "                betas = (training_args.adam_beta1, training_args.adam_beta2),\n",
    "                eps = training_args.adam_epsilon,\n",
    "                lr = training_args.learning_rate\n",
    "            )\n",
    "            trainer = Trainer(model = model, args = training_args, train_dataset = ds, optimizers = (adam_bnb_optim, None))\n",
    "            \n",
    "            del decay_parameters, optimizer_grouped_parameters, adam_bnb_optim\n",
    "        \n",
    "        else:\n",
    "            trainer = Trainer(model = model, args = training_args, train_dataset = ds)\n",
    "        \n",
    "        result = trainer.train()\n",
    "        print_summary(result)\n",
    "        \n",
    "        del trainer, training_args, result\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print()\n",
    "    print(\"_______________________torch.cuda.empty_cache()_______________________\")\n",
    "    print(f\"Allocated GPU memory: {torch.cuda.memory_allocated('cuda:0') / (1024 ** 3):.2f}GB\")\n",
    "    print(f\"Reserved GPU memory: {torch.cuda.memory_reserved('cuda:0') / (1024 ** 3):.2f}GB\")\n",
    "\n",
    "\n",
    "clean_training(training = False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:47:03.803974500Z",
     "start_time": "2023-08-01T03:47:03.801464100Z"
    }
   },
   "id": "66f9a773a9d652b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vanila Training\n",
    "\n",
    "default_args, batch_size = 4, Trainer를 통해 모델 학습"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f779588f7b4bad"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 110.7153, 'train_samples_per_second': 4.624, 'train_steps_per_second': 1.156, 'train_loss': 0.029618393629789352, 'epoch': 1.0}\n",
      "Time: 110.72\n",
      "Samples/second: 4.62\n",
      "GPU memory occupied: 12239 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "    per_device_train_batch_size = 4\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:19:15.758247200Z",
     "start_time": "2023-07-31T04:17:23.057984600Z"
    }
   },
   "id": "330000fa09cbc11f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "batch_size가 4인데 GPU의 전체 메모리 12GB를 거의 가득 채우고 있다.\n",
    "\n",
    "배치 크기가 클수록 모델 수렴 속도가 빨라지거나 최종 성능이 향상될 수 있다.\n",
    "\n",
    "따라서 이상적으로는 GPU 제한이 아닌 모델의 요구사항에 맞게 배치 크기를 조정하고 싶으나 모델의 크기보다 훨씬 더 많은 메모리를 사용하고 있다.\n",
    "\n",
    "그 이유를 더 잘 이해하기 위해 모델의 작동 및 메모리 요구사항을 살펴보자"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25fae2d3215650ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model`s Operations\n",
    "\n",
    "Transformers 아키텍처는 아래 3개 메인 그룹의 operation을 포함한다.\n",
    "\n",
    "1. Tensor Contractions, 텐소 축소\n",
    "    - Multi-Head Attention의 선형 레이어와 컴포넌트들은 모두 행렬X행렬의 배치 작업을 수행하는데 가장 compute-intensive한 부분이다.\n",
    "2. Statistical Normalizations\n",
    "    - Softmax와 레이어 정규화는 tensor contractions보다 덜 compute-intensive하지만 하나 이상의 축소 연산을 포함하며 그 결과는 맵을 통해 적용된다.\n",
    "3. Element-wise Operators\n",
    "    - biases, dropout, activations, residual connections는 그 외 가장 compute-intensive하지 않은 연산들이다.\n",
    "\n",
    "\n",
    "# Model`s Memory\n",
    "\n",
    "모델이 학습하는 동안 아래와 같은 수많은 컴포넌트가 사용되어 모델 크기에 비해 훨씬 많은 메모리가 사용된다.\n",
    "1. 모델 가중치\n",
    "2. optimizer states\n",
    "3. gradient\n",
    "4. forward activations saved for gradient computation\n",
    "5. temporary buffers\n",
    "6. functionality-specific memory\n",
    "\n",
    "AdamW를 사용하여 혼합정밀도를 사용하는 기존 모델은 학습시마다 파라미터당 18바이트의 메모리가 필요하다. 추론을 위해 optimizer states와 gradient가 없으므로 이를 빼면 혼합 정밀도 추론을 위한 모델 파라미터당 6바이트와 활성화 메모리로 끝난다.\n",
    "1. Model Weights\n",
    "    - FP32 훈련시 파라미터당 4바이트\n",
    "    - FP16 & FP32 혼합 정밀도 훈련시 파라미터당 6바이트\n",
    "2. Optimizer States\n",
    "    - 기존 AdamW와 같이 2state 유지하는 옵티마이저 사용시 파라미터당 8바이트\n",
    "    - bitsandbytes와 같은 8-bit 옵티마이저 사용시 파라미터당 2바이트\n",
    "    - momentum을 사용하는 SGD 옵티마이저와 같이 1state만 유지하는 경우 파라미터당 4바이트\n",
    "3. Gradients:\n",
    "    - gradient는 항상 FP32, 파라미터당 4바이트 할당\n",
    "4. Forward Activations:\n",
    "    - sequence length, hidden size, batch size 등 많은 요소에 의존\n",
    "5. Temporary Memory:\n",
    "    - 훈련시 사용되는 임시 변수들\n",
    "6. Fuctionality-specific memory:\n",
    "    - 특정 함수의 메모리 사용량\n",
    "    - ex.) generating text는 beam search를 사용하는데 input과 output의 복사본을 여러개 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f84383bbd535dc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Accumulation\n",
    "\n",
    "gradient accumulation step 동안 mini-batch 수행 후 gradient를 global gradient에 축적한다.\n",
    "\n",
    "훈련시간은 느려지나 훈련의 accuracy는 증가한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540fb6054f8b5c13"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 119.3135, 'train_samples_per_second': 4.291, 'train_steps_per_second': 1.073, 'train_loss': 1.2572745617944747e-06, 'epoch': 1.0}\n",
      "Time: 119.31\n",
      "Samples/second: 4.29\n",
      "GPU memory occupied: 8357 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# Trainer에게 TrainingArguments 객체를 전달할 때 gradient_accumulation_steps 값을 주는 것으로 쉽게 사용 사능하다.\n",
    "\n",
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:21:40.980234100Z",
     "start_time": "2023-07-31T04:19:41.549299900Z"
    }
   },
   "id": "ff8c337106ee5690"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### training 결과\n",
    "\n",
    "- 훈련시간 증가\n",
    "- 메모리 사용량 감소\n",
    "\n",
    "> 64개 batch 훈련이 필요하다면 batch size = 4, gradient accumulation size = 16이 같은 batch 크기를 가지면서 메모리는 훨씬 적게 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c1d0d373c7fe4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Checkpointing\n",
    "\n",
    "large model 훈련시 메모리가 부족할 수 있다.\n",
    "\n",
    "TrainingArguments에 gradient_checkpointing = true 값을 주고 더 효율적은 훈련이 가능하다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13c812f82ce9a382"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.3928, 'train_samples_per_second': 3.316, 'train_steps_per_second': 0.829, 'train_loss': 2.8638167393069125e-08, 'epoch': 1.0}\n",
      "Time: 154.39\n",
      "Samples/second: 3.32\n",
      "GPU memory occupied: 6872 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tgradient_checkpointing = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:27:11.000135800Z",
     "start_time": "2023-07-31T04:24:36.495883400Z"
    }
   },
   "id": "cb8149c3429dedfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### training 결과\n",
    "\n",
    "마찬가지로 gradient_checkpointing = False와 비교했을 때, 훈련시간은 증가했으나 메모리 사용량은 감소했다.\n",
    "\n",
    "일반적으로 약 20% 정도 느려진다고 한다.\n",
    "\n",
    "아래 혼합 정밀도 훈련에서 속도를 다시 보장할 수 있다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab96cf81793710c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Floating Data Types\n",
    "\n",
    "혼합 정밀도 훈련의 핵심은 모든 변수가 32비트에 저장될 필요는 없다는 것이다.\n",
    "- fp32(float32)\n",
    "- fp16(float16)\n",
    "- bf16(bfloat16)\n",
    "- tf32(CUDA internal data type)\n",
    "\n",
    "![](assets/tf32-bf16-fp16-fp32.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "676db13a69945d57"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.3202, 'train_samples_per_second': 3.318, 'train_steps_per_second': 0.829, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 154.32\n",
      "Samples/second: 3.32\n",
      "GPU memory occupied: 6871 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# FP16 Training\n",
    "\"\"\"\n",
    "- batch 4\n",
    "- gradient accumulation 적용\n",
    "- gradient checkpoint 사용\n",
    "\"\"\"\n",
    "\n",
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tgradient_checkpointing = True,\n",
    "\tfloat_precision = \"fp16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:31:13.306642600Z",
     "start_time": "2023-07-31T04:28:38.878398500Z"
    }
   },
   "id": "d33fe4d74060d4e7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.6933, 'train_samples_per_second': 3.31, 'train_steps_per_second': 0.827, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 154.69\n",
      "Samples/second: 3.31\n",
      "GPU memory occupied: 6871 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# BF16 Training\n",
    "\"\"\"\n",
    "    NVIDIA Ampere 장비나 더 최신의 H/W를 사용한다면 bf16 training이 가능하다.\n",
    "    정밀도는 fp16보다 떨어지나 더 큰 범위를 가질 수 있다.\n",
    "    \n",
    "    - fp16 최대값: 65535\n",
    "    - bf16 최대값: 3.39e+38\n",
    "\"\"\"\n",
    "\n",
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tgradient_checkpointing = True,\n",
    "\tfloat_precision = \"bf16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:34:06.519287100Z",
     "start_time": "2023-07-31T04:31:31.717252900Z"
    }
   },
   "id": "655cb54d41b13807"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.9592, 'train_samples_per_second': 3.304, 'train_steps_per_second': 0.826, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 154.96\n",
      "Samples/second: 3.30\n",
      "GPU memory occupied: 6889 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# TF32 Training\n",
    "\"\"\"\n",
    "    Ampere H/W는 tf32라는 개쩌는 data type을 사용한다.\n",
    "    fp32와 동일한 범위(8bit)를 갖고 있으나 fp16과 같은 정밀도(10bit)를 갖는다.\n",
    "    > total: sign(1 bit) + range(8 bit) + precision(10 bit) = 19bits\n",
    "    \n",
    "    코드에 아래 두 줄만 작성하면 pytorch가 자동으로 fp32를 tf32로 변환하여 훈련한다.\n",
    "    ```python\n",
    "    import torch\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    ```\n",
    "    \n",
    "    HuggingFace에서는 한 줄로 사용 가능하다.\n",
    "    ```python\n",
    "    tf32 = True\n",
    "    ```\n",
    "\"\"\"\n",
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tgradient_checkpointing = True,\n",
    "\tfloat_precision = \"bf16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:37:12.337998Z",
     "start_time": "2023-07-31T04:34:37.274216500Z"
    }
   },
   "id": "d67a40c995dc609a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Optimizer\n",
    "\n",
    "transformer model의 일반적인 optimizer는 Adam 또는 AdamW(Adam with weight decay)를 사용해왔다. 성능은 좋지만 추가적인 메모리 사용량이 있다.\n",
    "\n",
    "HuggingFace trainer model은 --optim flag를 통해 다양한 Optimizer를 지원한다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3df0f14d1fa05ae8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adafactor\n",
    "\n",
    "가중치 행렬의 각 요소에 대한 평균 대신, 집계된 정보만 저장하므로 메모리 사용량이 현저히 줄어듦.\n",
    "\n",
    "단, 수렴이 Adam의 수렴보다 느린 경우가 있음"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f264e5b428abba58"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 146.7754, 'train_samples_per_second': 3.488, 'train_steps_per_second': 0.872, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 146.78\n",
      "Samples/second: 3.49\n",
      "GPU memory occupied: 4771 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "\tper_device_train_batch_size = 4,\n",
    "\toptim = \"adafactor\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:57:26.869233600Z",
     "start_time": "2023-08-01T03:54:59.936905100Z"
    }
   },
   "id": "8d700b2a865a99aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "메모리 사용량이 현저히 줄어든게 보인다.\n",
    "\n",
    "앞의 다른 메스드들도 추가한 결과는 다음과 같다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bf055c099223b8f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 160.8274, 'train_samples_per_second': 3.184, 'train_steps_per_second': 0.796, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 160.83\n",
      "Samples/second: 3.18\n",
      "GPU memory occupied: 4532 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_checkpointing = True,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tfloat_precision = \"fp16\",\n",
    "\toptim = \"adafactor\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T04:00:23.816384300Z",
     "start_time": "2023-08-01T03:57:42.817813900Z"
    }
   },
   "id": "5b796e0f99f0a413"
  },
  {
   "cell_type": "markdown",
   "source": [
    "튜토리얼 보면 3배는 빨라지던데 왜징.."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61e8a443fbd1d9ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8-bit Adam\n",
    "\n",
    "Adafactor처럼 optimizer의 상태를 집계하는 대신, 8-bit Adam은 full state를 유지하고 양자화한다.\n",
    "\n",
    "이는 낮은 정밀도로 저장했다가 최적화 시에만 원래 정밀도로 계산해 메모리 사용량을 줄일 수 있다.\n",
    "\n",
    "Trainer에 플래그로 추가할 수 없어서 8-bit Adam optimizer(를 구현한 bitsandbytes)를 설치해 커스터마이징해야한다.\n",
    "\n",
    "- Linux: [여기](https://github.com/TimDettmers/bitsandbytes)서 설치\n",
    "- Windows: [여기](https://github.com/jllllll/bitsandbytes-windows-webui)서 설치\n",
    "\n",
    "System Env.\n",
    "- CONDA_PREFIX: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\n",
    "- 위 환경 변수는 아래 파일의 설치 위치\n",
    "    - Linux: \\*cuda\\*.so\n",
    "    - Windows: \\*cuda\\*.dll\n",
    "    - ex.) cudart64_110.dll 설치 위치\n",
    "\n",
    "설치 후 optimizer를 초기화해야 하는데 고려사항이 2개 있다.\n",
    "1. model`s parameters를 가중치 감쇠(weight decay)를 적용할 그룹과 적용하지 않을 그룹으로 나눈다.\n",
    "2. AdamW optimizer와 동일한 파라미터를 사용하기 위해 몇가지 전달인자에 대한 housekeeping이 필요하다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddc08279082641ab"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 101.0413, 'train_samples_per_second': 5.067, 'train_steps_per_second': 1.267, 'train_loss': 0.015763485804200172, 'epoch': 1.0}\n",
      "Time: 101.04\n",
      "Samples/second: 5.07\n",
      "GPU memory occupied: 10429 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "    per_device_train_batch_size = 4,\n",
    "    use_bnb = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:49:55.478700100Z",
     "start_time": "2023-08-01T03:48:13.304560Z"
    }
   },
   "id": "a278c58345db7d64"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 148.6336, 'train_samples_per_second': 3.445, 'train_steps_per_second': 0.861, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 148.63\n",
      "Samples/second: 3.44\n",
      "GPU memory occupied: 4889 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# 모든 메서드 적용\n",
    "\n",
    "clean_training(\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    gradient_checkpointing = True,\n",
    "    float_precision = \"fp16\",\n",
    "    use_bnb = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:52:32.924970200Z",
     "start_time": "2023-08-01T03:50:04.133656600Z"
    }
   },
   "id": "b5b40b044d0889e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "얘도 메서드 적용 안한게 더 빠르네..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ce57e94af78f204"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accelerate\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe933aac426ee18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a2593ba435b6e2cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc00e4aed111c7fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e75b93b267af5c56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cb83e1fa5981bb0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f1a2b2eddc6a8ca5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5379ce726c040cd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2221a83ed9577a77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cffa8e7799fdf67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f5c09cd85029b3a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9d5c10bee3d39c35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "87da4d5150bfbc53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d6998e114a29de58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "435ed30db76133c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e079209037ef53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e6ad1cacd88899da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ebc9d44be8233e7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "70a33e9c10687681"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3a592fa9103d8d78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6e2ab2e546cb16ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fee725b7ba7c8969"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7f68233de668226b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "50dfdecf9276f02c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1f8df933575f8b60"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f78c2bb24ba7dce1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aa4da075fe4d785c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7778d3a3458361ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5ebf789e84bae3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "70661543757e2d9c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66df0f9f38048bc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5047b39621ac9f58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3db634dad7595a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "65269ae7a99d26e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a82088a8e2f7454f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "be90046c7b164af3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d07a48fbbab6368f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aa8488084c5653a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "49a3a3f9b1550725"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b209fd0d5bf5c041"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1ab12b81ddd5395"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b9df2542933ce716"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f57dfcfb9995d718"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7ca9d4d41e204170"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aea46a7d3c7e8e4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "26997aa0ec48300"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "580a34cb00e85c75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3dd97057cd59c786"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3405af859615760a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dce98be0ca45fc56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "51de46b9f375f9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7bf080886bfa2898"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aab39fe30861adf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2a42001fc0c530e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6548f1f646773fbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d05781cc95e05c25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "14f0b7fc0c64a056"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c37a6b59903c754"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d45c31f88f3da84f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "33a72cd1b5285a88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "81ebe718b92de21f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a5c1324a3c62b293"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ce07405e3cd861"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2d9122bf2801e34f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "33d224b799cea72c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aeb7e06c74c59453"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c2f1f802228691e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "95a91bc8808f50a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "10958b7556b2de65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f42fa9e4dbc0a286"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8f14b872ba9915e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a2a179179c4cb642"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b885322073394cca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BetterTransformer: PyTorch-native transformer fastpath\n",
    "\n",
    "Inference 전용 Transformer.\n",
    "\n",
    "Transformer의 encoder, encoder layer, multi head attention을 다음 대표적인 두 방법으로 가속화.\n",
    "1. fused kernel: 효율 up\n",
    "2. exploiting sparsity in the inputs: pad_token과 같이 불필요한 부분을 생략\n",
    "\n",
    "\\* PyTorch 1.13 이상의 버전에서 호환\n",
    "`pip install accelerate optimum`\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30076b8f8a1c375c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e3d97f288d6d3cf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "29397515fba8f5b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "40b9e7acf0342133"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "512bfb6db9d65d73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d1908f36cdf04ca4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1fcfe48091907393"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================gc======================\n",
      "Allocated memory: 1.3GB\n",
      "GPU Reserved memory: 4.0GB\n",
      "======================empty cache======================\n",
      "Allocated memory: 1.3GB\n",
      "GPU Reserved memory: 1.3GB\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T03:36:51.237497Z",
     "start_time": "2023-07-31T03:36:51.149451600Z"
    }
   },
   "id": "17cad95620859be"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only assign an iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mclear_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[15], line 8\u001B[0m, in \u001B[0;36mclear_cache\u001B[1;34m(caches)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mclear_cache\u001B[39m(caches: List \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m caches \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m----> 8\u001B[0m         \u001B[43mcaches\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m     10\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "\u001B[1;31mTypeError\u001B[0m: can only assign an iterable"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T03:27:51.929520500Z",
     "start_time": "2023-07-31T03:27:51.915845500Z"
    }
   },
   "id": "3e70e679f6f235f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

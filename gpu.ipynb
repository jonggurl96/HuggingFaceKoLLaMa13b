{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HuggingFace with GPU\n",
    "###### 참고\n",
    "- [Hugging Face - Training on One GPU](https://huggingface.co/docs/transformers/perf_train_gpu_one)\n",
    "- [Hugging Face - Inference on One GPU](https://huggingface.co/docs/transformers/perf_infer_gpu_one)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad3a235cf8de9e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Efficient Training on a Single GPU\n",
    "\n",
    "더 적은 메모리 사용, 모델 학습 속도 상승, Trainer와 Accelerate가 통합되는 과정을 살펴본다.\n",
    "\n",
    "아래의 각 method들은 속도 또는 메모리 사용 면에서 향상된다.\n",
    "\n",
    "| Method | Speed | Memory | 비고 |\n",
    "| --- | --- | --- |\n",
    "| Gradient accumulation | No | Yes | 높은 batch size의 부하를 견디기 위해 모든 batch가 gradient를 Global Gradient에 누적시킨 뒤 한 번의 forward pass와 back propagation을 통해 전달 |\n",
    "| Gradient checkpointing | No | Yes | 모델 학습 시 모든 노드의 가중치를 저장하지 않고 check point의 가중치만 저장하여 속도가 느린 대신 메모리 사용량이 줄어듦 |\n",
    "| Mixed precision training | Yes | (No) | FP(Float Precision)16과 FP32를 혼용, FW / BW propagation은 FP16, 가중치를 업데이트하는 과정에서 다시 FP32로 변환해 메모리 사용을 줄이고 변환 과정의 오차로 인한 loss값도 줄임 |\n",
    "| Batch size | Yes | Yes | - |\n",
    "| Optimizer choice | Yes | Yes | - |\n",
    "| DataLoader | Yes | No | - |\n",
    "| Deepspeed Zero | No | Yes | 분산 학습 과정에서의 불필요한 메모리 중복을 제거하여 동시에 학습 가능한 파라미터의 수를 크레 늘릴 수 있는 새로운 병렬 최적화 도구 |\n",
    "\n",
    "### Libraries\n",
    "`pip install transformers datasets accelerate nvidia-ml-py3`\n",
    "\n",
    "- accelerate: 같은 PyTorch 코드라도 4줄만 추가해 더 쉽고 효율적으로 사용할 수 있도록 만들어주는 라이브러리\n",
    "- nvidia-ml-py3: 모델의 메모리 사용량을 파이썬에서 모니터링할 수 있는 라이브러리. 터미널의 nvidia-smi와 유사"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f29cace84313367"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# dummy data\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, dataset_size)\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T06:53:23.723456300Z",
     "start_time": "2023-07-28T06:53:21.717622700Z"
    }
   },
   "id": "b3d73676ac61b1fb"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 695 MB.\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용율 및 Trainer를 사용한 훈련 실행에 대한 요약 통계\n",
    "\n",
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "print_gpu_utilization()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T07:12:35.667135800Z",
     "start_time": "2023-07-28T07:12:35.661721Z"
    }
   },
   "id": "fa430cd847b5af6f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6450f5c694d43d2bce224e083627fa0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jongg\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13d6e986cfd346d5a358a8e75d67fd9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2116 MB.\n"
     ]
    }
   ],
   "source": [
    "# Load Model bert-large-uncased model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to(\"cuda\")\n",
    "print_gpu_utilization()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T07:19:40.088821900Z",
     "start_time": "2023-07-28T07:19:20.940258700Z"
    }
   },
   "id": "8191b6dcdf8c2e42"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# default training args\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T07:23:32.110118Z",
     "start_time": "2023-07-28T07:23:32.107609900Z"
    }
   },
   "id": "c1089701ef2e039f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vanila Training\n",
    "\n",
    "default_args, batch_size = 4, Trainer를 통해 모델 학습"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f779588f7b4bad"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 111.7234, 'train_samples_per_second': 4.583, 'train_steps_per_second': 1.146, 'train_loss': 0.019563835114240646, 'epoch': 1.0}\n",
      "Time: 111.72\n",
      "Samples/second: 4.58\n",
      "GPU memory occupied: 12183 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error() # error log activate\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size = 4, **default_args)\n",
    "trainer = Trainer(model = model, args = training_args, train_dataset = ds)\n",
    "result = trainer.train()\n",
    "print_summary(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T07:29:28.157068800Z",
     "start_time": "2023-07-28T07:27:33.870679200Z"
    }
   },
   "id": "330000fa09cbc11f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "batch_size가 4인데 GPU의 전체 메모리를 거의 가득 채우고 있다.\n",
    "\n",
    "배치 크기가 클수록 모델 수렴 속도가 빨라지거나 최종 성능이 향상될 수 있다.\n",
    "\n",
    "따라서 이상적으로는 GPU 제한이 아닌 모델의 요구사항에 맞게 배치 크기를 조정하고 싶으나 모델의 크기보다 훨씬 더 많은 메모리를 사용하고 있다.\n",
    "\n",
    "그 이유를 더 잘 이해하기 위해 모델의 작동 및 메모리 요구사항을 살펴보자"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25fae2d3215650ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model`s Operations\n",
    "\n",
    "Transformers 아키텍처는 아래 3개 메인 그룹의 operation을 포함한다.\n",
    "\n",
    "1. Tensor Contractions, 텐소 축소\n",
    "    - Multi-Head Attention의 선형 레이어와 컴포넌트들은 모두 행렬X행렬의 배치 작업을 수행하는데 가장 compute-intensive한 부분이다.\n",
    "2. Statistical Normalizations\n",
    "    - Softmax와 레이어 정규화는 tensor contractions보다 덜 compute-intensive하지만 하나 이상의 축소 연산을 포함하며 그 결과는 맵을 통해 적용된다.\n",
    "3. Element-wise Operators\n",
    "    - biases, dropout, activations, residual connections는 그 외 가장 compute-intensive하지 않은 연산들이다.\n",
    "\n",
    "\n",
    "# Model`s Memory\n",
    "\n",
    "모델이 학습하는 동안 아래와 같은 수많은 컴포넌트가 사용되어 모델 크기에 비해 훨씬 많은 메모리가 사용된다.\n",
    "1. 모델 가중치\n",
    "2. optimizer states\n",
    "3. gradient\n",
    "4. forward activations saved for gradient computation\n",
    "5. temporary buffers\n",
    "6. functionality-specific memory\n",
    "\n",
    "AdamW를 사용하여 혼합정밀도를 사용하는 기존 모델은 학습시마다 파라미터당 18바이트의 메모리가 필요하다. 추론을 위해 optimizer states와 gradient가 없으므로 이를 빼면 혼합 정밀도 추론을 위한 모델 파라미터당 6바이트와 활성화 메모리로 끝난다.\n",
    "1. Model Weights\n",
    "    - FP32 훈련시 파라미터당 4바이트\n",
    "    - FP16 & FP32 혼합 정밀도 훈련시 파라미터당 6바이트\n",
    "2. Optimizer States\n",
    "    - 기존 AdamW와 같이 2state 유지하는 옵티마이저 사용시 파라미터당 8바이트\n",
    "    - bitsandbytes와 같은 8-bit 옵티마이저 사용시 파라미터당 2바이트\n",
    "    - momentum을 사용하는 SGD 옵티마이저와 같이 1state만 유지하는 경우 파라미터당 4바이트\n",
    "3. Gradients:\n",
    "4. Forward Activations:\n",
    "5. Temporary Memory:\n",
    "6. Fuctionality-specific memory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f84383bbd535dc1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "52b5042bad8bb129"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff8c337106ee5690"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c0349c6c99fab1fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c004f395c47b4cd4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cb8149c3429dedfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e7a7459fc3abf03c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BetterTransformer: PyTorch-native transformer fastpath\n",
    "\n",
    "Inference 전용 Transformer.\n",
    "\n",
    "Transformer의 encoder, encoder layer, multi head attention을 다음 대표적인 두 방법으로 가속화.\n",
    "1. fused kernel: 효율 up\n",
    "2. exploiting sparsity in the inputs: pad_token과 같이 불필요한 부분을 생략\n",
    "\n",
    "\\* PyTorch 1.13 이상의 버전에서 호환\n",
    "`pip install accelerate optimum`\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30076b8f8a1c375c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e3d97f288d6d3cf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "29397515fba8f5b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "40b9e7acf0342133"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "512bfb6db9d65d73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d1908f36cdf04ca4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1fcfe48091907393"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17cad95620859be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3e70e679f6f235f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "900c62715ee62433"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "1331"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Garbage Collect\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T07:11:40.418002800Z",
     "start_time": "2023-07-28T07:11:40.412735100Z"
    }
   },
   "id": "368b67c3d2e615c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

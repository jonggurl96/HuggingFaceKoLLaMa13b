{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HuggingFace with GPU\n",
    "###### 참고\n",
    "- [Hugging Face - Training on One GPU](https://huggingface.co/docs/transformers/perf_train_gpu_one)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad3a235cf8de9e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Efficient Training on a Single GPU\n",
    "\n",
    "더 적은 메모리 사용, 모델 학습 속도 상승, Trainer와 Accelerate가 통합되는 과정을 살펴본다.\n",
    "\n",
    "아래의 각 method들은 속도 또는 메모리 사용 면에서 향상된다.\n",
    "\n",
    "| Method | Speed | Memory | 비고 |\n",
    "| --- | --- | --- |\n",
    "| Gradient accumulation | No | Yes | 높은 batch size의 부하를 견디기 위해 모든 batch가 gradient를 Global Gradient에 누적시킨 뒤 한 번의 forward pass와 back propagation을 통해 전달 |\n",
    "| Gradient checkpointing | No | Yes | 모델 학습 시 모든 노드의 가중치를 저장하지 않고 check point의 가중치만 저장하여 속도가 느린 대신 메모리 사용량이 줄어듦 |\n",
    "| Mixed precision training | Yes | (No) | FP(Float Precision)16과 FP32를 혼용, FW / BW propagation은 FP16, 가중치를 업데이트하는 과정에서 다시 FP32로 변환해 메모리 사용을 줄이고 변환 과정의 오차로 인한 loss값도 줄임 |\n",
    "| Batch size | Yes | Yes | - |\n",
    "| Optimizer choice | Yes | Yes | - |\n",
    "| DataLoader | Yes | No | - |\n",
    "| Deepspeed Zero | No | Yes | 분산 학습 과정에서의 불필요한 메모리 중복을 제거하여 동시에 학습 가능한 파라미터의 수를 크레 늘릴 수 있는 새로운 병렬 최적화 도구 |\n",
    "\n",
    "### Libraries\n",
    "`pip install transformers datasets accelerate nvidia-ml-py3`\n",
    "\n",
    "- accelerate: 같은 PyTorch 코드라도 4줄만 추가해 더 쉽고 효율적으로 사용할 수 있도록 만들어주는 라이브러리\n",
    "- nvidia-ml-py3: 모델의 메모리 사용량을 파이썬에서 모니터링할 수 있는 라이브러리. 터미널의 nvidia-smi와 유사"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f29cace84313367"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# dummy data\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "\n",
    "seq_len, dataset_size = 512, 512\n",
    "dummy_data = {\n",
    "    \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "    \"labels\": np.random.randint(0, 1, dataset_size)\n",
    "}\n",
    "ds = Dataset.from_dict(dummy_data)\n",
    "ds.set_format(\"pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:45:52.535521200Z",
     "start_time": "2023-08-01T03:45:50.974436400Z"
    }
   },
   "id": "b3d73676ac61b1fb"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 343 MB.\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용율 및 Trainer를 사용한 훈련 실행에 대한 요약 통계\n",
    "\n",
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "print_gpu_utilization()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:45:52.570276400Z",
     "start_time": "2023-08-01T03:45:52.536521300Z"
    }
   },
   "id": "fa430cd847b5af6f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1734 MB.\n"
     ]
    }
   ],
   "source": [
    "# Load Model bert-large-uncased model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-large-uncased\").to(\"cuda\")\n",
    "print_gpu_utilization()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:45:56.179237800Z",
     "start_time": "2023-08-01T03:45:53.910833300Z"
    }
   },
   "id": "8191b6dcdf8c2e42"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# default training args\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\"\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:45:56.180744100Z",
     "start_time": "2023-08-01T03:45:56.179237800Z"
    }
   },
   "id": "c1089701ef2e039f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training 함수"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fea30953735fcbf7"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.training_args import OptimizerNames\n",
    "from typing import Union\n",
    "import bitsandbytes as bnb\n",
    "import torch.cuda\n",
    "import gc\n",
    "\n",
    "logging.set_verbosity_error()  # error log activate\n",
    "\n",
    "\n",
    "def get_training_args(\n",
    "        per_device_train_batch_size: int = 8,\n",
    "\t\tgradient_accumulation_steps: int = 1,\n",
    "\t\tgradient_checkpointing: bool = False,\n",
    "\t\toptim: Union[OptimizerNames, str] = \"adamw_hf\",\n",
    "\t\tfloat_precision: str = None,\n",
    "\t\tdeepspeed: str = None,\n",
    ") -> TrainingArguments:\n",
    "\ttraining_args = TrainingArguments(\n",
    "\t\tper_device_train_batch_size = per_device_train_batch_size,\n",
    "\t\tgradient_accumulation_steps = gradient_accumulation_steps,\n",
    "\t\tgradient_checkpointing = gradient_checkpointing,\n",
    "\t\toptim = optim,\n",
    "        deepspeed = deepspeed,\n",
    "\t\t**default_args)\n",
    "\t\n",
    "\tif float_precision == \"fp16\":\n",
    "\t\ttraining_args.fp16 = True\n",
    "\telif float_precision == \"bf16\":\n",
    "\t\ttraining_args.bf16 = True\n",
    "\telif float_precision == \"tf32\":\n",
    "\t\ttraining_args.tf32 = True\n",
    "\t\n",
    "\treturn training_args\n",
    "\n",
    "\n",
    "def get_adam_bnb_optim(training_args: TrainingArguments) -> bnb.optim.AdamW8bit:\n",
    "    decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
    "    decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
    "            \"weight_decay\": training_args.weight_decay\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
    "            \"weight_decay\": 0.0\n",
    "        }\n",
    "    ]\n",
    "    adam_bnb_optim = bnb.optim.AdamW8bit(\n",
    "        optimizer_grouped_parameters,\n",
    "        betas = (training_args.adam_beta1, training_args.adam_beta2),\n",
    "        eps = training_args.adam_epsilon,\n",
    "        lr = training_args.learning_rate\n",
    "    )\n",
    "    \n",
    "    del decay_parameters, optimizer_grouped_parameters, training_args\n",
    "    \n",
    "    return adam_bnb_optim\n",
    "\n",
    "\n",
    "def clean_training(\n",
    "\t\tper_device_train_batch_size: int = 8,\n",
    "\t\tgradient_accumulation_steps: int = 1,\n",
    "\t\tgradient_checkpointing: bool = False,\n",
    "\t\toptim: Union[OptimizerNames, str] = \"adamw_hf\",\n",
    "\t\tfloat_precision: str = None,\n",
    "\t\tdeepspeed: str = None,\n",
    "        training: bool = True,\n",
    "        use_bnb: bool = False\n",
    "):\n",
    "    if training:\n",
    "        training_args = get_training_args(\n",
    "\t        per_device_train_batch_size = per_device_train_batch_size,\n",
    "            gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "            gradient_checkpointing = gradient_checkpointing,\n",
    "            optim = optim,\n",
    "            float_precision = float_precision,\n",
    "            deepspeed = deepspeed\n",
    "\t    )\n",
    "        \n",
    "        if float_precision == \"fp16\":\n",
    "            training_args.fp16 = True\n",
    "        elif float_precision == \"bf16\":\n",
    "            training_args.bf16 = True\n",
    "        elif float_precision == \"tf32\":\n",
    "            training_args.tf32 = True\n",
    "            \n",
    "        # trainer = get_trainer(args = training_args, use_bnb = use_bnb)\n",
    "        \n",
    "        if use_bnb:\n",
    "            adam_bnb_optim = get_adam_bnb_optim(training_args = training_args)\n",
    "            trainer = Trainer(model = model, args = training_args, train_dataset = ds, optimizers = (adam_bnb_optim, None))\n",
    "            \n",
    "            del adam_bnb_optim\n",
    "        \n",
    "        else:\n",
    "            trainer = Trainer(model = model, args = training_args, train_dataset = ds)\n",
    "        \n",
    "        result = trainer.train()\n",
    "        print_summary(result)\n",
    "        \n",
    "        del trainer, training_args, result\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print()\n",
    "    print(\"_______________________torch.cuda.empty_cache()_______________________\")\n",
    "    print(f\"Allocated GPU memory: {torch.cuda.memory_allocated('cuda:0') / (1024 ** 3):.2f}GB\")\n",
    "    print(f\"Reserved GPU memory: {torch.cuda.memory_reserved('cuda:0') / (1024 ** 3):.2f}GB\")\n",
    "\n",
    "\n",
    "clean_training(training = False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T07:37:15.586558500Z",
     "start_time": "2023-08-01T07:37:15.582995200Z"
    }
   },
   "id": "66f9a773a9d652b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vanila Training\n",
    "\n",
    "default_args, batch_size = 4, Trainer를 통해 모델 학습"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6f779588f7b4bad"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jongg\\PycharmProjects\\HuggingFaceKoLLaMa13b\\venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 110.7153, 'train_samples_per_second': 4.624, 'train_steps_per_second': 1.156, 'train_loss': 0.029618393629789352, 'epoch': 1.0}\n",
      "Time: 110.72\n",
      "Samples/second: 4.62\n",
      "GPU memory occupied: 12239 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "    per_device_train_batch_size = 4\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:19:15.758247200Z",
     "start_time": "2023-07-31T04:17:23.057984600Z"
    }
   },
   "id": "330000fa09cbc11f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "batch_size가 4인데 GPU의 전체 메모리 12GB를 거의 가득 채우고 있다.\n",
    "\n",
    "배치 크기가 클수록 모델 수렴 속도가 빨라지거나 최종 성능이 향상될 수 있다.\n",
    "\n",
    "따라서 이상적으로는 GPU 제한이 아닌 모델의 요구사항에 맞게 배치 크기를 조정하고 싶으나 모델의 크기보다 훨씬 더 많은 메모리를 사용하고 있다.\n",
    "\n",
    "그 이유를 더 잘 이해하기 위해 모델의 작동 및 메모리 요구사항을 살펴보자"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25fae2d3215650ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model`s Operations\n",
    "\n",
    "Transformers 아키텍처는 아래 3개 메인 그룹의 operation을 포함한다.\n",
    "\n",
    "1. Tensor Contractions, 텐소 축소\n",
    "    - Multi-Head Attention의 선형 레이어와 컴포넌트들은 모두 행렬X행렬의 배치 작업을 수행하는데 가장 compute-intensive한 부분이다.\n",
    "2. Statistical Normalizations\n",
    "    - Softmax와 레이어 정규화는 tensor contractions보다 덜 compute-intensive하지만 하나 이상의 축소 연산을 포함하며 그 결과는 맵을 통해 적용된다.\n",
    "3. Element-wise Operators\n",
    "    - biases, dropout, activations, residual connections는 그 외 가장 compute-intensive하지 않은 연산들이다.\n",
    "\n",
    "\n",
    "# Model`s Memory\n",
    "\n",
    "모델이 학습하는 동안 아래와 같은 수많은 컴포넌트가 사용되어 모델 크기에 비해 훨씬 많은 메모리가 사용된다.\n",
    "1. 모델 가중치\n",
    "2. optimizer states\n",
    "3. gradient\n",
    "4. forward activations saved for gradient computation\n",
    "5. temporary buffers\n",
    "6. functionality-specific memory\n",
    "\n",
    "AdamW를 사용하여 혼합정밀도를 사용하는 기존 모델은 학습시마다 파라미터당 18바이트의 메모리가 필요하다. 추론을 위해 optimizer states와 gradient가 없으므로 이를 빼면 혼합 정밀도 추론을 위한 모델 파라미터당 6바이트와 활성화 메모리로 끝난다.\n",
    "1. Model Weights\n",
    "    - FP32 훈련시 파라미터당 4바이트\n",
    "    - FP16 & FP32 혼합 정밀도 훈련시 파라미터당 6바이트\n",
    "2. Optimizer States\n",
    "    - 기존 AdamW와 같이 2state 유지하는 옵티마이저 사용시 파라미터당 8바이트\n",
    "    - bitsandbytes와 같은 8-bit 옵티마이저 사용시 파라미터당 2바이트\n",
    "    - momentum을 사용하는 SGD 옵티마이저와 같이 1state만 유지하는 경우 파라미터당 4바이트\n",
    "3. Gradients:\n",
    "    - gradient는 항상 FP32, 파라미터당 4바이트 할당\n",
    "4. Forward Activations:\n",
    "    - sequence length, hidden size, batch size 등 많은 요소에 의존\n",
    "5. Temporary Memory:\n",
    "    - 훈련시 사용되는 임시 변수들\n",
    "6. Fuctionality-specific memory:\n",
    "    - 특정 함수의 메모리 사용량\n",
    "    - ex.) generating text는 beam search를 사용하는데 input과 output의 복사본을 여러개 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f84383bbd535dc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Accumulation\n",
    "\n",
    "gradient accumulation step 동안 mini-batch 수행 후 gradient를 global gradient에 축적한다.\n",
    "\n",
    "훈련시간은 느려지나 훈련의 accuracy는 증가한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "540fb6054f8b5c13"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 119.3135, 'train_samples_per_second': 4.291, 'train_steps_per_second': 1.073, 'train_loss': 1.2572745617944747e-06, 'epoch': 1.0}\n",
      "Time: 119.31\n",
      "Samples/second: 4.29\n",
      "GPU memory occupied: 8357 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# Trainer에게 TrainingArguments 객체를 전달할 때 gradient_accumulation_steps 값을 주는 것으로 쉽게 사용 사능하다.\n",
    "\n",
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:21:40.980234100Z",
     "start_time": "2023-07-31T04:19:41.549299900Z"
    }
   },
   "id": "ff8c337106ee5690"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### training 결과\n",
    "\n",
    "- 훈련시간 증가\n",
    "- 메모리 사용량 감소\n",
    "\n",
    "> 64개 batch 훈련이 필요하다면 batch size = 4, gradient accumulation size = 16이 같은 batch 크기를 가지면서 메모리는 훨씬 적게 사용한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c1d0d373c7fe4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Checkpointing\n",
    "\n",
    "large model 훈련시 메모리가 부족할 수 있다.\n",
    "\n",
    "TrainingArguments에 gradient_checkpointing = true 값을 주고 더 효율적은 훈련이 가능하다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13c812f82ce9a382"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.3928, 'train_samples_per_second': 3.316, 'train_steps_per_second': 0.829, 'train_loss': 2.8638167393069125e-08, 'epoch': 1.0}\n",
      "Time: 154.39\n",
      "Samples/second: 3.32\n",
      "GPU memory occupied: 6872 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tgradient_checkpointing = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:27:11.000135800Z",
     "start_time": "2023-07-31T04:24:36.495883400Z"
    }
   },
   "id": "cb8149c3429dedfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### training 결과\n",
    "\n",
    "마찬가지로 gradient_checkpointing = False와 비교했을 때, 훈련시간은 증가했으나 메모리 사용량은 감소했다.\n",
    "\n",
    "일반적으로 약 20% 정도 느려진다고 한다.\n",
    "\n",
    "아래 혼합 정밀도 훈련에서 속도를 다시 보장할 수 있다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab96cf81793710c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Floating Data Types\n",
    "\n",
    "혼합 정밀도 훈련의 핵심은 모든 변수가 32비트에 저장될 필요는 없다는 것이다.\n",
    "- fp32(float32)\n",
    "- fp16(float16)\n",
    "- bf16(bfloat16)\n",
    "- tf32(CUDA internal data type)\n",
    "\n",
    "![](assets/tf32-bf16-fp16-fp32.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "676db13a69945d57"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.3202, 'train_samples_per_second': 3.318, 'train_steps_per_second': 0.829, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 154.32\n",
      "Samples/second: 3.32\n",
      "GPU memory occupied: 6871 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# FP16 Training\n",
    "\"\"\"\n",
    "- batch 4\n",
    "- gradient accumulation 적용\n",
    "- gradient checkpoint 사용\n",
    "\"\"\"\n",
    "\n",
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tgradient_checkpointing = True,\n",
    "\tfloat_precision = \"fp16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:31:13.306642600Z",
     "start_time": "2023-07-31T04:28:38.878398500Z"
    }
   },
   "id": "d33fe4d74060d4e7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.6933, 'train_samples_per_second': 3.31, 'train_steps_per_second': 0.827, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 154.69\n",
      "Samples/second: 3.31\n",
      "GPU memory occupied: 6871 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# BF16 Training\n",
    "\"\"\"\n",
    "    NVIDIA Ampere 장비나 더 최신의 H/W를 사용한다면 bf16 training이 가능하다.\n",
    "    정밀도는 fp16보다 떨어지나 더 큰 범위를 가질 수 있다.\n",
    "    \n",
    "    - fp16 최대값: 65535\n",
    "    - bf16 최대값: 3.39e+38\n",
    "\"\"\"\n",
    "\n",
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tgradient_checkpointing = True,\n",
    "\tfloat_precision = \"bf16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:34:06.519287100Z",
     "start_time": "2023-07-31T04:31:31.717252900Z"
    }
   },
   "id": "655cb54d41b13807"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 154.9592, 'train_samples_per_second': 3.304, 'train_steps_per_second': 0.826, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 154.96\n",
      "Samples/second: 3.30\n",
      "GPU memory occupied: 6889 MB.\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# TF32 Training\n",
    "\"\"\"\n",
    "    Ampere H/W는 tf32라는 개쩌는 data type을 사용한다.\n",
    "    fp32와 동일한 범위(8bit)를 갖고 있으나 fp16과 같은 정밀도(10bit)를 갖는다.\n",
    "    > total: sign(1 bit) + range(8 bit) + precision(10 bit) = 19bits\n",
    "    \n",
    "    코드에 아래 두 줄만 작성하면 pytorch가 자동으로 fp32를 tf32로 변환하여 훈련한다.\n",
    "    ```python\n",
    "    import torch\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    ```\n",
    "    \n",
    "    HuggingFace에서는 한 줄로 사용 가능하다.\n",
    "    ```python\n",
    "    tf32 = True\n",
    "    ```\n",
    "\"\"\"\n",
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tgradient_checkpointing = True,\n",
    "\tfloat_precision = \"bf16\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T04:37:12.337998Z",
     "start_time": "2023-07-31T04:34:37.274216500Z"
    }
   },
   "id": "d67a40c995dc609a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Optimizer\n",
    "\n",
    "transformer model의 일반적인 optimizer는 Adam 또는 AdamW(Adam with weight decay)를 사용해왔다. 성능은 좋지만 추가적인 메모리 사용량이 있다.\n",
    "\n",
    "HuggingFace trainer model은 --optim flag를 통해 다양한 Optimizer를 지원한다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3df0f14d1fa05ae8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Adafactor\n",
    "\n",
    "가중치 행렬의 각 요소에 대한 평균 대신, 집계된 정보만 저장하므로 메모리 사용량이 현저히 줄어듦.\n",
    "\n",
    "단, 수렴이 Adam의 수렴보다 느린 경우가 있음"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f264e5b428abba58"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 146.7754, 'train_samples_per_second': 3.488, 'train_steps_per_second': 0.872, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 146.78\n",
      "Samples/second: 3.49\n",
      "GPU memory occupied: 4771 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "\tper_device_train_batch_size = 4,\n",
    "\toptim = \"adafactor\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:57:26.869233600Z",
     "start_time": "2023-08-01T03:54:59.936905100Z"
    }
   },
   "id": "8d700b2a865a99aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "메모리 사용량이 현저히 줄어든게 보인다.\n",
    "\n",
    "앞의 다른 메스드들도 추가한 결과는 다음과 같다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bf055c099223b8f"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 160.8274, 'train_samples_per_second': 3.184, 'train_steps_per_second': 0.796, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 160.83\n",
      "Samples/second: 3.18\n",
      "GPU memory occupied: 4532 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "\tper_device_train_batch_size = 1,\n",
    "\tgradient_checkpointing = True,\n",
    "\tgradient_accumulation_steps = 4,\n",
    "\tfloat_precision = \"fp16\",\n",
    "\toptim = \"adafactor\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T04:00:23.816384300Z",
     "start_time": "2023-08-01T03:57:42.817813900Z"
    }
   },
   "id": "5b796e0f99f0a413"
  },
  {
   "cell_type": "markdown",
   "source": [
    "튜토리얼 보면 3배는 빨라지던데 왜징.."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61e8a443fbd1d9ae"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8-bit Adam\n",
    "\n",
    "Adafactor처럼 optimizer의 상태를 집계하는 대신, 8-bit Adam은 full state를 유지하고 양자화한다.\n",
    "\n",
    "이는 낮은 정밀도로 저장했다가 최적화 시에만 원래 정밀도로 계산해 메모리 사용량을 줄일 수 있다.\n",
    "\n",
    "Trainer에 플래그로 추가할 수 없어서 8-bit Adam optimizer(를 구현한 bitsandbytes)를 설치해 커스터마이징해야한다.\n",
    "\n",
    "- Linux: [여기](https://github.com/TimDettmers/bitsandbytes)서 설치\n",
    "- Windows: [여기](https://github.com/jllllll/bitsandbytes-windows-webui)서 설치\n",
    "\n",
    "System Env.\n",
    "- CONDA_PREFIX: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\n",
    "- 위 환경 변수는 아래 파일의 설치 위치\n",
    "    - Linux: \\*cuda\\*.so\n",
    "    - Windows: \\*cuda\\*.dll\n",
    "    - ex.) cudart64_110.dll 설치 위치\n",
    "\n",
    "설치 후 optimizer를 초기화해야 하는데 고려사항이 2개 있다.\n",
    "1. model`s parameters를 가중치 감쇠(weight decay)를 적용할 그룹과 적용하지 않을 그룹으로 나눈다.\n",
    "2. AdamW optimizer와 동일한 파라미터를 사용하기 위해 몇가지 전달인자에 대한 housekeeping이 필요하다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddc08279082641ab"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 101.0413, 'train_samples_per_second': 5.067, 'train_steps_per_second': 1.267, 'train_loss': 0.015763485804200172, 'epoch': 1.0}\n",
      "Time: 101.04\n",
      "Samples/second: 5.07\n",
      "GPU memory occupied: 10429 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "clean_training(\n",
    "    per_device_train_batch_size = 4,\n",
    "    use_bnb = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:49:55.478700100Z",
     "start_time": "2023-08-01T03:48:13.304560Z"
    }
   },
   "id": "a278c58345db7d64"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 148.6336, 'train_samples_per_second': 3.445, 'train_steps_per_second': 0.861, 'train_loss': 0.0, 'epoch': 1.0}\n",
      "Time: 148.63\n",
      "Samples/second: 3.44\n",
      "GPU memory occupied: 4889 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 1.27GB\n",
      "Reserved GPU memory: 1.30GB\n"
     ]
    }
   ],
   "source": [
    "# 모든 메서드 적용\n",
    "\n",
    "clean_training(\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    gradient_checkpointing = True,\n",
    "    float_precision = \"fp16\",\n",
    "    use_bnb = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T03:52:32.924970200Z",
     "start_time": "2023-08-01T03:50:04.133656600Z"
    }
   },
   "id": "b5b40b044d0889e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "얘도 메서드 적용 안한게 더 빠르네..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ce57e94af78f204"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accelerate\n",
    "\n",
    "지금까지 Trainer를 사용했지만 보다 유연한 대안은 accelerate를 사용하는 것이다..?(그럼 Trainer 왜 쓴거임;;)\n",
    "\n",
    "Accelerate를 사용하면 훈련 루프를 완전히 제어할 수 있고 약간의 수정으로 순수 PyTorch에서 루프를 작성할 수 있다.\n",
    "\n",
    "따라서 코드 변경 없이 CPU, GPU, TPU, multi-GPU 등 다양한 인프라에서 쉽게 확장 가능하다.\n",
    "\n",
    "추가)) 딥러닝 학습시 데이터나 모델이 크고 리소스는 한계가 있을 때 이를 해결해주는 라이브러리로 Multi-GPU 분산 구성을 100% 활용, deepspeed를 활용할 수 있음"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6fe933aac426ee18"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 5177 MB.\n",
      "\n",
      "_______________________torch.cuda.empty_cache()_______________________\n",
      "Allocated GPU memory: 2.51GB\n",
      "Reserved GPU memory: 2.55GB\n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "training_args = get_training_args(\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_checkpointing = True,\n",
    "    float_precision = \"fp16\"\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(ds, batch_size = training_args.per_device_train_batch_size)\n",
    "\n",
    "if training_args.fp16:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "accelerator = Accelerator(mixed_precision = \"fp16\")\n",
    "\n",
    "adam_bnb_optim = get_adam_bnb_optim(training_args = training_args)\n",
    "\n",
    "model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)\n",
    "\n",
    "model.train()\n",
    "for step, batch in enumerate(dataloader, start = 1):\n",
    "    loss = model(**batch).loss\n",
    "    loss /= training_args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss)\n",
    "    if step % training_args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "print_gpu_utilization()\n",
    "\n",
    "del training_args, dataloader, accelerator, adam_bnb_optim, optimizer\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print()\n",
    "print(\"_______________________torch.cuda.empty_cache()_______________________\")\n",
    "print(f\"Allocated GPU memory: {torch.cuda.memory_allocated('cuda:0') / (1024 ** 3):.2f}GB\")\n",
    "print(f\"Reserved GPU memory: {torch.cuda.memory_reserved('cuda:0') / (1024 ** 3):.2f}GB\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T07:47:31.388450700Z",
     "start_time": "2023-08-01T07:46:00.314021300Z"
    }
   },
   "id": "a2593ba435b6e2cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader\n",
    "\n",
    "학습속도에 지대한 영향을 미치는 것 중 하나는 GPU가 조종 가능한 최고 스피드인데 데이터를 읽을 때 병목현상이 일어나거나 GPU 효율이 떨어질 수 있다.\n",
    "\n",
    "- DataLoader(pin_memory=True, ...): CPU의 pinned memory에 미리 데이터를 로딩하고 사용시 GPU에 빠르게 전달\n",
    "- DataLoader(num_workers=4, ...): 학습시 GPU utilization 상태를 보는 동안 worker를 지정해 데이터를 미리 로딩 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "935828530a9d8a4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DeepSpeed ZeRO\n",
    "### DeepSpeed Zero Redundancy Optmizer\n",
    "- 모델이 단일 GPU에 적합하며 작은 배치 그키에 맞는 충분한 공간이 있을 경우 사용할 필요 없음\n",
    "- 모델이 단일 GPU에 맞지 않거나 작은 배치와 어울리지 않는 경우 DeepSpeed ZeRO + CPU 오프로드를 사용하고 훨씬 더 큰 모델의 경우 NVMe 오프로드를 사용\n",
    "\n",
    "딥러닝 모델의 분산 학습(DDP)에서 모든 GPU가 모델, parameter, gradient, optimizer state를 복제해서 가지고 있어야 했고 이를 해결한게 DeepSpeed ZeRO\n",
    "- Stage 1: Optimizer State Partitioning -> 메모리 4배 감소\n",
    "- Stage 2: Optimizer State Partitioning + Gradient Partitioning -> 메모리 8배 감소\n",
    "- Stage 3: Optimizer State Partitioning + Gradient Partitioning + Parameter Partitioning -> GPU 개수와 비례 감소 (GPU 32개 사용 -> 메모리 32배 감소), OOM 발생 시 에러를 올리지 않고 나머지를 하드디스크에 보내고 CPU로 연산\n",
    "- Stage Infinity\n",
    "\n",
    "다 좋은데 Windows에선 좀 복잡하니 패스.."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a1066622b37628"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using PyTorch Native Attention\n",
    "\n",
    "PyTorch 2.0 버전에 릴리즈:\n",
    "torch.nn.functional.scaled_dot_product_attention, (SDPA)\n",
    "> memory-efficient attention과 flash attention 지원\n",
    "\n",
    "optimum package 설치 후 내부 모듈을 교체하여 PyTorch의 기본 Attention을 다음과 같이 사용할 수 있다.\n",
    "`model = model.to_bettertransformer()`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e080d61a2daa517"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
